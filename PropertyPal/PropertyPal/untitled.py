# -*- coding: utf-8 -*-
"""Untitled.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l_Wdhr0jFw1ds5gJuR8cHm-08jjdK6Vf
"""



import mlflow
import pandas as pd
from langchain.llms import HuggingFacePipeline
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.text_splitter import CharacterTextSplitter
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig
import torch
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory
from langchain.chains import LLMChain
from langchain.schema import Document
import anvil.server

class CustomOutputParser:
    def parse(self, response: str):
        parts = response.split('[/INST]')
        if len(parts) > 1:
            return parts[-1].strip()
        return response.strip()

class LLMRAGModel:
    def __init__(self, llm_name="NousResearch/Llama-2-7b-chat-hf"):
        self.load_model(llm_name)
        self.setup_pipeline()
        self.retriever = self.buildRetrieval()

    def load_model(self, model_name):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16
        )
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map='auto',
            torch_dtype=torch.float16,
            quantization_config=bnb_config
        )

    def setup_pipeline(self):
        self.llmPipeline = pipeline(
            "text-generation",
            model=self.model,
            tokenizer=self.tokenizer,
            torch_dtype=torch.float16,
            device_map="auto",
            max_new_tokens=400,
            do_sample=True,
            top_k=50,
            num_return_sequences=1,
            eos_token_id=self.tokenizer.eos_token_id
        )
        self.llm = HuggingFacePipeline(
            pipeline=self.llmPipeline, 
            model_kwargs={'temperature': 0.1, 'max_length': 350, 'top_k': 10}
        )

    def generate_text(self, prompt):
        return self.llmPipeline(prompt)

    def getPromptFromTemplate(self):
        system_prompt = """You are PropertyPal, a specialized assistant designed to provide property recommendations based exclusively on the data given to you. Your responses should be based solely on the user's specific queries regarding property details and should reflect only the data available in the dataset.

1. If the user asks for property recommendations, provide the following details for each property that matches their criteria:
   - Location
   - Number of bedrooms
   - Number of bathrooms
   - Property size
   - Price

2. Do not include any additional details such as property IDs, extra information, or suggestions beyond what is present in the dataset.

3. If the user's input is a generic greeting or does not pertain to property recommendations, respond with a neutral message indicating that you can help with property queries and ask them to specify their requirements.

Your responses should be concise and focused on the data provided, with no additional or extraneous details."""

        B_INST, E_INST = "[INST]", "[/INST]"
        B_SYS, E_SYS = " << SYS>>\n", "\n << /SYS>>\n\n"

        SYSTEM_PROMPT1 = B_SYS + system_prompt + E_SYS

        instruction = """
        History: {history} \n
        Context: {context} \n
        User: {question}"""

        prompt_template = B_INST + SYSTEM_PROMPT1 + instruction + E_INST

        prompt = PromptTemplate(input_variables=["history", "question", "context"], template=prompt_template)

        return prompt

    def buildRetrieval(self, model_name="sentence-transformers/all-MiniLM-L6-v2", csv_file=None):
        if csv_file is None:
            csv_file = r"Users/hadeedrauf6/properties.csv"

        df = pd.read_csv(csv_file)

        documents = df.apply(lambda row: Document(
            page_content=f"Property is located in {row['location']}. The price of the property is {row['price']}. It has {row['bedrooms']} bedrooms and {row['bathrooms']} bathrooms. The size of the property is {row['size']} square feet.",
            metadata=row.to_dict()
        ), axis=1).tolist()

        texts = [doc.page_content for doc in documents]

        embeddings = HuggingFaceEmbeddings(model_name=model_name)
        text_splitter = CharacterTextSplitter(chunk_size=250, chunk_overlap=100, separator="feet.")
        texts = text_splitter.split_documents([Document(page_content=text) for text in texts])

        db = FAISS.from_documents(texts, embeddings)
        retriever = db.as_retriever()

        return retriever

    def getNewChain(self):
        prompt = self.getPromptFromTemplate()
        memory = ConversationBufferMemory(input_key="question", memory_key="history", max_len=5)
        llm_chain = LLMChain(prompt=prompt, llm=self.llm, verbose=True, memory=memory)
        rag_chain = (
            {"context": self.retriever, "question": lambda x: x}
            | llm_chain
        )
        return rag_chain

def getAnswer(chain, question):
    response = chain.invoke(question)
    response_text = response.get('text', 'No response text found')
    parser = CustomOutputParser()
    return parser.parse(response_text)

def main():
    mlflow.autolog()
    model_instance = LLMRAGModel()
    chain = model_instance.getNewChain()

    anvil.server.connect("server_GHCXZYPN7FYGDDV2IRDBJFIG-5EMKN7KUZ3BZKLXO")

    @anvil.server.callable
    def get_bot_response(prompt):
        response_text = getAnswer(chain, prompt)
        return response_text

   # anvil.server.wait_forever()

if __name__ == "__main__":
    main()
